{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shakibibnashameem/Documents/Practice/bert/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import mps\n",
    "from transformers import BertTokenizer, BertForTokenClassification, BertConfig\n",
    "\n",
    "\n",
    "device = 'mps' if mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/shakibibnashameem/Documents/Practice/bert/bert-ner/data/ner_data.csv'\n",
    "data = pd.read_csv(path, encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence #    1000616\n",
       "Word               10\n",
       "POS                 0\n",
       "Tag                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOB tag count\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tag\n",
       "O        887908\n",
       "B-geo     37644\n",
       "B-tim     20333\n",
       "B-org     20143\n",
       "I-per     17251\n",
       "B-per     16990\n",
       "I-org     16784\n",
       "B-gpe     15870\n",
       "I-geo      7414\n",
       "I-tim      6528\n",
       "B-art       402\n",
       "B-eve       308\n",
       "I-art       297\n",
       "I-eve       253\n",
       "B-nat       201\n",
       "I-gpe       198\n",
       "I-nat        51\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert iob tags to base tag\n",
    "data['base_tag'] = data['Tag'].apply(lambda x: x.split('-')[-1])\n",
    "\n",
    "freqs = data['Tag'].value_counts()\n",
    "print(\"IOB tag count\")\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique base tag: ['org', 'per', 'O', 'tim', 'gpe', 'nat', 'eve', 'art', 'geo']\n"
     ]
    }
   ],
   "source": [
    "iob_tags = []\n",
    "for t, f in zip(freqs.index, freqs):\n",
    "    iob_tags.append(t)\n",
    "\n",
    "unique_tag =  []\n",
    "for tag in iob_tags:\n",
    "    s = tag.split('-')\n",
    "    unique_tag.append(s[-1])\n",
    "\n",
    "unique_tag = list(set(unique_tag))\n",
    "print(f'Unique base tag: {unique_tag}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "base_tag\n",
       "O      887908\n",
       "geo     45058\n",
       "org     36927\n",
       "per     34241\n",
       "tim     26861\n",
       "gpe     16068\n",
       "art       699\n",
       "eve       561\n",
       "nat       252\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frequency of unique tags\n",
    "data['base_tag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# art eve nat is not defined properly, removing them\n",
    "to_remove = ['art','nat','eve']\n",
    "\n",
    "data = data[~data.base_tag.isin(to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id lable lookup dict for model\n",
    "\n",
    "labels = data['Tag'].value_counts().index\n",
    "\n",
    "label2id = {}\n",
    "id2label = {}\n",
    "for idx, label in enumerate(labels):\n",
    "    label2id[label] = idx\n",
    "    id2label[idx] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Families of soldiers killed in the conflict jo...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-per,O,O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They marched from the Houses of Parliament to ...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,B-geo,I-geo,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Police put the number of marchers at 10,000 wh...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The protest comes on the eve of the annual con...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,B-geo,O,O,B-org,I-org,O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  Thousands of demonstrators have marched throug...   \n",
       "1  Families of soldiers killed in the conflict jo...   \n",
       "2  They marched from the Houses of Parliament to ...   \n",
       "3  Police put the number of marchers at 10,000 wh...   \n",
       "4  The protest comes on the eve of the annual con...   \n",
       "\n",
       "                                         word_labels  \n",
       "0  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...  \n",
       "1  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-per,O,O,...  \n",
       "2                O,O,O,O,O,O,O,O,O,O,O,B-geo,I-geo,O  \n",
       "3                      O,O,O,O,O,O,O,O,O,O,O,O,O,O,O  \n",
       "4  O,O,O,O,O,O,O,O,O,O,O,B-geo,O,O,B-org,I-org,O,...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forming sentence from tokens\n",
    "data['sentence'] = data[['Sentence #','Word', \"Tag\"]].groupby(['Sentence #'])[\"Word\"].transform(lambda x: ' '.join(x))\n",
    "data['word_labels'] = data[['Sentence #','Word', \"Tag\"]].groupby(['Sentence #'])[\"Tag\"].transform(lambda x: ','.join(x))\n",
    "\n",
    "data = data[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # step 1: tokenize (and adapt corresponding labels)\n",
    "        sentence = self.data.sentence[index]  \n",
    "        word_labels = self.data.word_labels[index]  \n",
    "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
    "        \n",
    "        # step 2: add special tokens (and corresponding labels)\n",
    "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
    "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
    "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
    "\n",
    "        # step 3: truncating/padding\n",
    "        maxlen = self.max_len\n",
    "\n",
    "        if (len(tokenized_sentence) > maxlen):\n",
    "          # truncate\n",
    "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "          labels = labels[:maxlen]\n",
    "        else:\n",
    "          # pad\n",
    "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
    "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
    "\n",
    "        # step 4: obtain the attention mask\n",
    "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
    "        \n",
    "        # step 5: convert tokens to input ids\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "\n",
    "        label_ids = [label2id[label] for label in labels]\n",
    "        # the following line is deprecated\n",
    "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
    "        \n",
    "        return {\n",
    "              'ids': torch.tensor(ids, dtype=torch.long),\n",
    "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
    "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 1e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "MODEL_PATH = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (47610, 2)\n",
      "TRAIN Dataset: (38088, 2)\n",
      "TEST Dataset: (9522, 2)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_dataset = data.sample(frac=train_size,random_state=200)\n",
    "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]       O\n",
      "and         O\n",
      "the         O\n",
      "u           B-geo\n",
      ".           B-geo\n",
      "s           B-geo\n",
      ".           B-geo\n",
      "military    O\n",
      "said        O\n",
      "a           O\n",
      "roadside    O\n",
      "bomb        O\n",
      "blast       O\n",
      "in          O\n",
      "east        B-geo\n",
      "baghdad     I-geo\n",
      "killed      O\n",
      "an          O\n",
      "american    B-gpe\n",
      "soldier     O\n",
      ".           O\n",
      "[SEP]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n"
     ]
    }
   ],
   "source": [
    "# print the first 30 tokens and corresponding labels\n",
    "idx = 67\n",
    "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[idx][\"ids\"][:50]), training_set[idx][\"targets\"][:50]):\n",
    "  print('{0:10}  {1}'.format(token, id2label[label.item()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(MODEL_PATH,\n",
    "                                                   num_labels=len(id2label),\n",
    "                                                   id2label=id2label,\n",
    "                                                   label2id=label2id)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5228, device='mps:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = training_set[0][\"ids\"].unsqueeze(0)\n",
    "mask = training_set[0][\"mask\"].unsqueeze(0)\n",
    "targets = training_set[0][\"targets\"].unsqueeze(0)\n",
    "ids = ids.to(device)\n",
    "mask = mask.to(device)\n",
    "targets = targets.to(device)\n",
    "outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "initial_loss = outputs[0]\n",
    "initial_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 11])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_logits = outputs[1]\n",
    "tr_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        \n",
    "        ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "        loss, tr_logits = outputs.loss, outputs.logits\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "        \n",
    "        if idx % 100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
    "           \n",
    "        # compute training accuracy\n",
    "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        tr_preds.extend(predictions)\n",
    "        tr_labels.extend(targets)\n",
    "        \n",
    "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training loss per 100 training steps: 2.6173088550567627\n",
      "Training loss per 100 training steps: 0.4547475338867395\n",
      "Training loss per 100 training steps: 0.29590264963569923\n",
      "Training loss per 100 training steps: 0.2319516947746673\n",
      "Training loss per 100 training steps: 0.19598995233674596\n",
      "Training loss per 100 training steps: 0.17142078681419531\n",
      "Training loss per 100 training steps: 0.1542610641663426\n",
      "Training loss per 100 training steps: 0.1406472508660046\n",
      "Training loss per 100 training steps: 0.12975703217682152\n",
      "Training loss per 100 training steps: 0.12070289586353365\n",
      "Training loss per 100 training steps: 0.1130760846958197\n",
      "Training loss per 100 training steps: 0.10708191913242142\n",
      "Training loss per 100 training steps: 0.10158915661173026\n",
      "Training loss per 100 training steps: 0.09728717789351711\n",
      "Training loss per 100 training steps: 0.09326924405909602\n",
      "Training loss per 100 training steps: 0.0896497611763039\n",
      "Training loss per 100 training steps: 0.08659891296715605\n",
      "Training loss per 100 training steps: 0.0839381986445496\n",
      "Training loss per 100 training steps: 0.08142543771432952\n",
      "Training loss per 100 training steps: 0.07934813568322502\n",
      "Training loss per 100 training steps: 0.07734255125987688\n",
      "Training loss per 100 training steps: 0.07540304928511356\n",
      "Training loss per 100 training steps: 0.0736030413853178\n",
      "Training loss per 100 training steps: 0.07185837912259106\n",
      "Training loss per 100 training steps: 0.07015249554774973\n",
      "Training loss per 100 training steps: 0.06851577344325381\n",
      "Training loss per 100 training steps: 0.06730543126950814\n",
      "Training loss per 100 training steps: 0.06606339293522419\n",
      "Training loss per 100 training steps: 0.06500434155076522\n",
      "Training loss per 100 training steps: 0.06390173889209266\n",
      "Training loss per 100 training steps: 0.06269703318511707\n",
      "Training loss per 100 training steps: 0.061607139697921594\n",
      "Training loss per 100 training steps: 0.06079459330332528\n",
      "Training loss per 100 training steps: 0.05999716463982207\n",
      "Training loss per 100 training steps: 0.059181405056271535\n",
      "Training loss per 100 training steps: 0.05847478710016929\n",
      "Training loss per 100 training steps: 0.0578131062743921\n",
      "Training loss per 100 training steps: 0.05718116722081256\n",
      "Training loss per 100 training steps: 0.056516464437073616\n",
      "Training loss per 100 training steps: 0.0559377925912975\n",
      "Training loss per 100 training steps: 0.05524660520620378\n",
      "Training loss per 100 training steps: 0.054696539238709464\n",
      "Training loss per 100 training steps: 0.05414322494722114\n",
      "Training loss per 100 training steps: 0.053573358237487975\n",
      "Training loss per 100 training steps: 0.05309020114033981\n",
      "Training loss per 100 training steps: 0.05264847081081803\n",
      "Training loss per 100 training steps: 0.05210773991640741\n",
      "Training loss per 100 training steps: 0.0516816646629865\n",
      "Training loss per 100 training steps: 0.05120091183830805\n",
      "Training loss per 100 training steps: 0.050887341488579256\n",
      "Training loss per 100 training steps: 0.05045653280611048\n",
      "Training loss per 100 training steps: 0.050107191781132446\n",
      "Training loss per 100 training steps: 0.049760331945856696\n",
      "Training loss per 100 training steps: 0.049414727557645984\n",
      "Training loss per 100 training steps: 0.049045986534957876\n",
      "Training loss per 100 training steps: 0.048649247547931485\n",
      "Training loss per 100 training steps: 0.048339981835895766\n",
      "Training loss per 100 training steps: 0.04808906088480867\n",
      "Training loss per 100 training steps: 0.04777648320806363\n",
      "Training loss per 100 training steps: 0.04747381013061672\n",
      "Training loss per 100 training steps: 0.04724494270431008\n",
      "Training loss per 100 training steps: 0.046986753669883925\n",
      "Training loss per 100 training steps: 0.04665208604043756\n",
      "Training loss per 100 training steps: 0.04639818571465473\n",
      "Training loss per 100 training steps: 0.04616839039721108\n",
      "Training loss per 100 training steps: 0.045895232214479435\n",
      "Training loss per 100 training steps: 0.04557246146526634\n",
      "Training loss per 100 training steps: 0.04529740991154569\n",
      "Training loss per 100 training steps: 0.045098430033751066\n",
      "Training loss per 100 training steps: 0.0448347474297824\n",
      "Training loss per 100 training steps: 0.04456619843096313\n",
      "Training loss per 100 training steps: 0.04431379517464791\n",
      "Training loss per 100 training steps: 0.04407903775282264\n",
      "Training loss per 100 training steps: 0.04382990831493925\n",
      "Training loss per 100 training steps: 0.04358909198370774\n",
      "Training loss per 100 training steps: 0.04335730149398373\n",
      "Training loss per 100 training steps: 0.04318624601076733\n",
      "Training loss per 100 training steps: 0.04298294093210113\n",
      "Training loss per 100 training steps: 0.04278016533909924\n",
      "Training loss per 100 training steps: 0.04255288802513422\n",
      "Training loss per 100 training steps: 0.042375332009159716\n",
      "Training loss per 100 training steps: 0.04220959339770257\n",
      "Training loss per 100 training steps: 0.042036368909893575\n",
      "Training loss per 100 training steps: 0.041849457032924905\n",
      "Training loss per 100 training steps: 0.0416749224006649\n",
      "Training loss per 100 training steps: 0.04156951871092489\n",
      "Training loss per 100 training steps: 0.04138169100963496\n",
      "Training loss per 100 training steps: 0.04117597609389182\n",
      "Training loss per 100 training steps: 0.041051388148168076\n",
      "Training loss per 100 training steps: 0.04090239568775517\n",
      "Training loss per 100 training steps: 0.04069189028229837\n",
      "Training loss per 100 training steps: 0.04055039126226895\n",
      "Training loss per 100 training steps: 0.040402909774537095\n",
      "Training loss per 100 training steps: 0.040287829282843864\n",
      "Training loss per 100 training steps: 0.04015794251657732\n",
      "Training loss per 100 training steps: 0.04002108112313161\n",
      "Training loss epoch: 0.039985346965484635\n",
      "Training accuracy epoch: 0.9490511007116836\n",
      "Training epoch: 2\n",
      "Training loss per 100 training steps: 0.003680316498503089\n",
      "Training loss per 100 training steps: 0.024840454220262327\n",
      "Training loss per 100 training steps: 0.025392459907461265\n",
      "Training loss per 100 training steps: 0.02459179174510431\n",
      "Training loss per 100 training steps: 0.02468393421016269\n",
      "Training loss per 100 training steps: 0.024276745870387362\n",
      "Training loss per 100 training steps: 0.024090736001036898\n",
      "Training loss per 100 training steps: 0.02401396943638719\n",
      "Training loss per 100 training steps: 0.02413160953434276\n",
      "Training loss per 100 training steps: 0.023991399667359893\n",
      "Training loss per 100 training steps: 0.023783622802615253\n",
      "Training loss per 100 training steps: 0.02385460310912717\n",
      "Training loss per 100 training steps: 0.023754802647406943\n",
      "Training loss per 100 training steps: 0.023989210444705478\n",
      "Training loss per 100 training steps: 0.024082969294518277\n",
      "Training loss per 100 training steps: 0.02407523780219042\n",
      "Training loss per 100 training steps: 0.023876186355221016\n",
      "Training loss per 100 training steps: 0.023508396387259135\n",
      "Training loss per 100 training steps: 0.023690184735906217\n",
      "Training loss per 100 training steps: 0.02369184515749823\n",
      "Training loss per 100 training steps: 0.023786760406391317\n",
      "Training loss per 100 training steps: 0.023778325226634426\n",
      "Training loss per 100 training steps: 0.023566496606566255\n",
      "Training loss per 100 training steps: 0.023517445964434763\n",
      "Training loss per 100 training steps: 0.023443647368925208\n",
      "Training loss per 100 training steps: 0.023447217754897245\n",
      "Training loss per 100 training steps: 0.023324851880684047\n",
      "Training loss per 100 training steps: 0.023204256141782974\n",
      "Training loss per 100 training steps: 0.023156103383938954\n",
      "Training loss per 100 training steps: 0.023314945264791057\n",
      "Training loss per 100 training steps: 0.023210878523397885\n",
      "Training loss per 100 training steps: 0.02325895249882021\n",
      "Training loss per 100 training steps: 0.02309452129882573\n",
      "Training loss per 100 training steps: 0.023031116034758166\n",
      "Training loss per 100 training steps: 0.022895975111896193\n",
      "Training loss per 100 training steps: 0.022771774641492964\n",
      "Training loss per 100 training steps: 0.02277119443579156\n",
      "Training loss per 100 training steps: 0.022770893212726438\n",
      "Training loss per 100 training steps: 0.02273573712813556\n",
      "Training loss per 100 training steps: 0.022666593485230157\n",
      "Training loss per 100 training steps: 0.02270997013293081\n",
      "Training loss per 100 training steps: 0.022734270819155942\n",
      "Training loss per 100 training steps: 0.02264442929884604\n",
      "Training loss per 100 training steps: 0.02264989031031629\n",
      "Training loss per 100 training steps: 0.02268688848431203\n",
      "Training loss per 100 training steps: 0.022629447704698182\n",
      "Training loss per 100 training steps: 0.022656534400987215\n",
      "Training loss per 100 training steps: 0.022654104984312633\n",
      "Training loss per 100 training steps: 0.02270228222079845\n",
      "Training loss per 100 training steps: 0.0227717600820452\n",
      "Training loss per 100 training steps: 0.022807040763512004\n",
      "Training loss per 100 training steps: 0.022780275303116788\n",
      "Training loss per 100 training steps: 0.02269498626416763\n",
      "Training loss per 100 training steps: 0.022624435051662835\n",
      "Training loss per 100 training steps: 0.022598934609327225\n",
      "Training loss per 100 training steps: 0.022609872334951795\n",
      "Training loss per 100 training steps: 0.022559200639878704\n",
      "Training loss per 100 training steps: 0.0225457306644446\n",
      "Training loss per 100 training steps: 0.02249381638003469\n",
      "Training loss per 100 training steps: 0.022428131971085473\n",
      "Training loss per 100 training steps: 0.022438777268467713\n",
      "Training loss per 100 training steps: 0.02240850253175838\n",
      "Training loss per 100 training steps: 0.02239724807993414\n",
      "Training loss per 100 training steps: 0.022437557201194325\n",
      "Training loss per 100 training steps: 0.022426238640465432\n",
      "Training loss per 100 training steps: 0.022384058020495406\n",
      "Training loss per 100 training steps: 0.022370446194748532\n",
      "Training loss per 100 training steps: 0.022336707912777344\n",
      "Training loss per 100 training steps: 0.022332855074077278\n",
      "Training loss per 100 training steps: 0.022314423279033675\n",
      "Training loss per 100 training steps: 0.02233982562314861\n",
      "Training loss per 100 training steps: 0.022341846396225374\n",
      "Training loss per 100 training steps: 0.02237246198653235\n",
      "Training loss per 100 training steps: 0.02233972058816419\n",
      "Training loss per 100 training steps: 0.02229591741987749\n",
      "Training loss per 100 training steps: 0.022287606462989867\n",
      "Training loss per 100 training steps: 0.022270419006412412\n",
      "Training loss per 100 training steps: 0.022259803027779176\n",
      "Training loss per 100 training steps: 0.02224321996851329\n",
      "Training loss per 100 training steps: 0.02222824223501627\n",
      "Training loss per 100 training steps: 0.02223183728154642\n",
      "Training loss per 100 training steps: 0.02221282079317374\n",
      "Training loss per 100 training steps: 0.02216611328392964\n",
      "Training loss per 100 training steps: 0.02214736314821515\n",
      "Training loss per 100 training steps: 0.022185745006104398\n",
      "Training loss per 100 training steps: 0.022190745653365155\n",
      "Training loss per 100 training steps: 0.022177370035681897\n",
      "Training loss per 100 training steps: 0.0221745416198815\n",
      "Training loss per 100 training steps: 0.02215835052527848\n",
      "Training loss per 100 training steps: 0.022142545098153627\n",
      "Training loss per 100 training steps: 0.022119015904218876\n",
      "Training loss per 100 training steps: 0.02211124768042969\n",
      "Training loss per 100 training steps: 0.022108786223114893\n",
      "Training loss per 100 training steps: 0.022129164426884004\n",
      "Training loss per 100 training steps: 0.02211642906991255\n",
      "Training loss per 100 training steps: 0.022134766982924624\n",
      "Training loss epoch: 0.02213153159866422\n",
      "Training accuracy epoch: 0.965988272370778\n",
      "Training epoch: 3\n",
      "Training loss per 100 training steps: 0.007755155675113201\n",
      "Training loss per 100 training steps: 0.01737157581143524\n",
      "Training loss per 100 training steps: 0.016224368954360004\n",
      "Training loss per 100 training steps: 0.01639392711411685\n",
      "Training loss per 100 training steps: 0.01600021549085088\n",
      "Training loss per 100 training steps: 0.01665018739698347\n",
      "Training loss per 100 training steps: 0.016782915974814544\n",
      "Training loss per 100 training steps: 0.016566613309801796\n",
      "Training loss per 100 training steps: 0.016394517994923263\n",
      "Training loss per 100 training steps: 0.016459150619822432\n",
      "Training loss per 100 training steps: 0.016680555774183193\n",
      "Training loss per 100 training steps: 0.01670344063863505\n",
      "Training loss per 100 training steps: 0.01678482948890232\n",
      "Training loss per 100 training steps: 0.016750882201463695\n",
      "Training loss per 100 training steps: 0.016889102218919757\n",
      "Training loss per 100 training steps: 0.016965892539479225\n",
      "Training loss per 100 training steps: 0.016883774489449537\n",
      "Training loss per 100 training steps: 0.01688252808753719\n",
      "Training loss per 100 training steps: 0.01675529240484547\n",
      "Training loss per 100 training steps: 0.016623857876275256\n",
      "Training loss per 100 training steps: 0.016593986079807155\n",
      "Training loss per 100 training steps: 0.01662734777181297\n",
      "Training loss per 100 training steps: 0.01670084655924108\n",
      "Training loss per 100 training steps: 0.01662133619942623\n",
      "Training loss per 100 training steps: 0.016614964000842367\n",
      "Training loss per 100 training steps: 0.016641460308784488\n",
      "Training loss per 100 training steps: 0.016717396751604982\n",
      "Training loss per 100 training steps: 0.016706774763724633\n",
      "Training loss per 100 training steps: 0.016721336414101493\n",
      "Training loss per 100 training steps: 0.01673256532845662\n",
      "Training loss per 100 training steps: 0.016686568141729516\n",
      "Training loss per 100 training steps: 0.016718758697706105\n",
      "Training loss per 100 training steps: 0.016666058045054305\n",
      "Training loss per 100 training steps: 0.016707272272135778\n",
      "Training loss per 100 training steps: 0.01669723381587406\n",
      "Training loss per 100 training steps: 0.01686115414534274\n",
      "Training loss per 100 training steps: 0.016902619012881554\n",
      "Training loss per 100 training steps: 0.01696062617506405\n",
      "Training loss per 100 training steps: 0.017013443272905136\n",
      "Training loss per 100 training steps: 0.01705524868377501\n",
      "Training loss per 100 training steps: 0.017089877205109523\n",
      "Training loss per 100 training steps: 0.017081172129166086\n",
      "Training loss per 100 training steps: 0.017118954555384376\n",
      "Training loss per 100 training steps: 0.017160182372176157\n",
      "Training loss per 100 training steps: 0.017193742049774318\n",
      "Training loss per 100 training steps: 0.017148346277905818\n",
      "Training loss per 100 training steps: 0.017204204710471654\n",
      "Training loss per 100 training steps: 0.017249701982448937\n",
      "Training loss per 100 training steps: 0.017266599516096128\n",
      "Training loss per 100 training steps: 0.017256641598471907\n",
      "Training loss per 100 training steps: 0.017209894424612748\n",
      "Training loss per 100 training steps: 0.01720902952461858\n",
      "Training loss per 100 training steps: 0.01723707341188625\n",
      "Training loss per 100 training steps: 0.0172572722079307\n",
      "Training loss per 100 training steps: 0.017231438905570638\n",
      "Training loss per 100 training steps: 0.0172599228529184\n",
      "Training loss per 100 training steps: 0.017262741637190014\n",
      "Training loss per 100 training steps: 0.017288981655332904\n",
      "Training loss per 100 training steps: 0.017304956688917215\n",
      "Training loss per 100 training steps: 0.017261035995956322\n",
      "Training loss per 100 training steps: 0.017223183965299968\n",
      "Training loss per 100 training steps: 0.01723806595551226\n",
      "Training loss per 100 training steps: 0.01724316580565389\n",
      "Training loss per 100 training steps: 0.01721145123050247\n",
      "Training loss per 100 training steps: 0.017229736134597787\n",
      "Training loss per 100 training steps: 0.01722982705956431\n",
      "Training loss per 100 training steps: 0.017227079163661507\n",
      "Training loss per 100 training steps: 0.017211752051538517\n",
      "Training loss per 100 training steps: 0.017194036213700286\n",
      "Training loss per 100 training steps: 0.017172431922964228\n",
      "Training loss per 100 training steps: 0.017161739971033575\n",
      "Training loss per 100 training steps: 0.017141233339436908\n",
      "Training loss per 100 training steps: 0.017137457196462137\n",
      "Training loss per 100 training steps: 0.01714155713434551\n",
      "Training loss per 100 training steps: 0.017115026752287904\n",
      "Training loss per 100 training steps: 0.01709660950065378\n",
      "Training loss per 100 training steps: 0.01707849589881123\n",
      "Training loss per 100 training steps: 0.017088581670479316\n",
      "Training loss per 100 training steps: 0.017083596519390003\n",
      "Training loss per 100 training steps: 0.017110856346129785\n",
      "Training loss per 100 training steps: 0.017087765005688146\n",
      "Training loss per 100 training steps: 0.017061657542974795\n",
      "Training loss per 100 training steps: 0.017095880612985788\n",
      "Training loss per 100 training steps: 0.017055861504816767\n",
      "Training loss per 100 training steps: 0.01706520054986057\n",
      "Training loss per 100 training steps: 0.017070928802646188\n",
      "Training loss per 100 training steps: 0.017068621614871552\n",
      "Training loss per 100 training steps: 0.017086264310343893\n",
      "Training loss per 100 training steps: 0.017063817253868855\n",
      "Training loss per 100 training steps: 0.017059192254382634\n",
      "Training loss per 100 training steps: 0.017046851720002944\n",
      "Training loss per 100 training steps: 0.01707235968452194\n",
      "Training loss per 100 training steps: 0.017029410755929553\n",
      "Training loss per 100 training steps: 0.017031545829861914\n",
      "Training loss per 100 training steps: 0.017038760878805993\n",
      "Training loss per 100 training steps: 0.017013531169816312\n",
      "Training loss epoch: 0.017002006808466297\n",
      "Training accuracy epoch: 0.9728531388748266\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['mask'].to(device, dtype = torch.long)\n",
    "            targets = batch['targets'].to(device, dtype = torch.long)\n",
    "            \n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            loss, eval_logits = outputs.loss, outputs.logits\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += targets.size(0)\n",
    "        \n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "              \n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            eval_labels.extend(targets)\n",
    "            eval_preds.extend(predictions)\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "    \n",
    "    #print(eval_labels)\n",
    "    #print(eval_preds)\n",
    "\n",
    "    labels = [id2label[id.item()] for id in eval_labels]\n",
    "    predictions = [id2label[id.item()] for id in eval_preds]\n",
    "\n",
    "    #print(labels)\n",
    "    #print(predictions)\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.001311454107053578\n",
      "Validation loss per 100 evaluation steps: 0.02690354802943337\n",
      "Validation loss per 100 evaluation steps: 0.026023789825893037\n",
      "Validation loss per 100 evaluation steps: 0.024191063664700755\n",
      "Validation loss per 100 evaluation steps: 0.024093230907248627\n",
      "Validation loss per 100 evaluation steps: 0.025557606428323082\n",
      "Validation loss per 100 evaluation steps: 0.026847256001378213\n",
      "Validation loss per 100 evaluation steps: 0.026777546216097747\n",
      "Validation loss per 100 evaluation steps: 0.027215735167654534\n",
      "Validation loss per 100 evaluation steps: 0.02690986261961964\n",
      "Validation loss per 100 evaluation steps: 0.026261705657505754\n",
      "Validation loss per 100 evaluation steps: 0.026586227521983332\n",
      "Validation loss per 100 evaluation steps: 0.02620035082363423\n",
      "Validation loss per 100 evaluation steps: 0.025977186875779927\n",
      "Validation loss per 100 evaluation steps: 0.025470469970036918\n",
      "Validation loss per 100 evaluation steps: 0.02512105940720947\n",
      "Validation loss per 100 evaluation steps: 0.02522031508091593\n",
      "Validation loss per 100 evaluation steps: 0.025212503907741362\n",
      "Validation loss per 100 evaluation steps: 0.025333430911013078\n",
      "Validation loss per 100 evaluation steps: 0.0251930639397759\n",
      "Validation loss per 100 evaluation steps: 0.025571067073813403\n",
      "Validation loss per 100 evaluation steps: 0.02560539903582454\n",
      "Validation loss per 100 evaluation steps: 0.02539410161672528\n",
      "Validation loss per 100 evaluation steps: 0.025287380017462377\n",
      "Validation loss per 100 evaluation steps: 0.02544828160863721\n",
      "Validation loss per 100 evaluation steps: 0.025316934606507677\n",
      "Validation loss per 100 evaluation steps: 0.02532654486283051\n",
      "Validation loss per 100 evaluation steps: 0.025224442351461727\n",
      "Validation loss per 100 evaluation steps: 0.025136452098558817\n",
      "Validation loss per 100 evaluation steps: 0.02503327149877266\n",
      "Validation loss per 100 evaluation steps: 0.02513006717020276\n",
      "Validation loss per 100 evaluation steps: 0.02520832761652469\n",
      "Validation loss per 100 evaluation steps: 0.02525876061871103\n",
      "Validation loss per 100 evaluation steps: 0.025122028168457832\n",
      "Validation loss per 100 evaluation steps: 0.02502250943514941\n",
      "Validation loss per 100 evaluation steps: 0.02489674766085593\n",
      "Validation loss per 100 evaluation steps: 0.024923565832016534\n",
      "Validation loss per 100 evaluation steps: 0.024900824938849472\n",
      "Validation loss per 100 evaluation steps: 0.024857427223247366\n",
      "Validation loss per 100 evaluation steps: 0.0248422936607572\n",
      "Validation loss per 100 evaluation steps: 0.024914256258444414\n",
      "Validation loss per 100 evaluation steps: 0.025142576735392535\n",
      "Validation loss per 100 evaluation steps: 0.02513754856652523\n",
      "Validation loss per 100 evaluation steps: 0.025020717939862196\n",
      "Validation loss per 100 evaluation steps: 0.025043473541229048\n",
      "Validation loss per 100 evaluation steps: 0.02496417216733955\n",
      "Validation loss per 100 evaluation steps: 0.02488698441341219\n",
      "Validation loss per 100 evaluation steps: 0.02487464079740337\n",
      "Validation Loss: 0.02484567352607236\n",
      "Validation Accuracy: 0.9656012624105635\n"
     ]
    }
   ],
   "source": [
    "labels, predictions = valid(model, testing_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/shakibibnashameem/Documents/Practice/bert/bert-ner/artifacts/v_3/tokenizer/tokenizer_config.json',\n",
       " '/Users/shakibibnashameem/Documents/Practice/bert/bert-ner/artifacts/v_3/tokenizer/special_tokens_map.json',\n",
       " '/Users/shakibibnashameem/Documents/Practice/bert/bert-ner/artifacts/v_3/tokenizer/vocab.txt',\n",
       " '/Users/shakibibnashameem/Documents/Practice/bert/bert-ner/artifacts/v_3/tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model & tokenizer\n",
    "\n",
    "path = '/Users/shakibibnashameem/Documents/Practice/bert/bert-ner/artifacts/v_3/'\n",
    "\n",
    "model.save_pretrained(path+'bert_trained')\n",
    "tokenizer.save_pretrained(path+'tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         geo       0.86      0.87      0.86     11585\n",
      "         gpe       0.97      0.91      0.94      3467\n",
      "         org       0.72      0.69      0.71      6785\n",
      "         per       0.79      0.80      0.79      5270\n",
      "         tim       0.84      0.84      0.84      4457\n",
      "\n",
      "   micro avg       0.83      0.82      0.82     31564\n",
      "   macro avg       0.83      0.82      0.83     31564\n",
      "weighted avg       0.83      0.82      0.82     31564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(classification_report([labels], [predictions]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india has a capital called mumbai . on wednesday , the president will give a presentation\n",
      "['B-geo', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"India has a capital called Mumbai. On wednesday, the president will give a presentation\"\n",
    "\n",
    "inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "\n",
    "# move to gpu\n",
    "ids = inputs[\"input_ids\"].to('cpu')\n",
    "mask = inputs[\"attention_mask\"].to('cpu')\n",
    "# forward pass\n",
    "outputs = model(ids, mask)\n",
    "logits = outputs[0]\n",
    "\n",
    "active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "token_predictions = [id2label[i] for i in flattened_predictions.cpu().numpy()]\n",
    "wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "\n",
    "word_level_predictions = []\n",
    "for pair in wp_preds:\n",
    "  if (pair[0].startswith(\" ##\")) or (pair[0] in ['[CLS]', '[SEP]', '[PAD]']):\n",
    "    # skip prediction\n",
    "    continue\n",
    "  else:\n",
    "    word_level_predictions.append(pair[1])\n",
    "\n",
    "# we join tokens, if they are not special ones\n",
    "str_rep = \" \".join([t[0] for t in wp_preds if t[0] not in ['[CLS]', '[SEP]', '[PAD]']]).replace(\" ##\", \"\")\n",
    "print(str_rep)\n",
    "print(word_level_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'org',\n",
       "  'score': np.float32(0.5601732),\n",
       "  'word': 'ni',\n",
       "  'start': 11,\n",
       "  'end': 13},\n",
       " {'entity_group': 'per',\n",
       "  'score': np.float32(0.7349655),\n",
       "  'word': '##els',\n",
       "  'start': 13,\n",
       "  'end': 16},\n",
       " {'entity_group': 'geo',\n",
       "  'score': np.float32(0.9530471),\n",
       "  'word': 'new york',\n",
       "  'start': 21,\n",
       "  'end': 29}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task=\"token-classification\", model=model.to(\"cpu\"), tokenizer=tokenizer, aggregation_strategy=\"simple\", device='mps')\n",
    "pipe(\"My name is Niels and New York is a city\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"><br>        The annual summit was held in the breathtaking  \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #ccf2ff, #b3e6ff); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Himalayas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">geo</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #ccffee, #b3ffdd); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nepal\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">gpe</span>\n",
       "</mark>\n",
       " and organized by the \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #fff4cc, #ffe6b3); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    United Nations\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">org</span>\n",
       "</mark>\n",
       ". <br>        Distinguished speakers, including \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #e3d8fd, #fbd8f5); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Professor Williams\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">per</span>\n",
       "</mark>\n",
       ", discussed climate change impacts during the <br>        session on \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #ffcccc, #ffb3b3); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    January 8th\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">tim</span>\n",
       "</mark>\n",
       ", with attendees from \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #ccf2ff, #b3e6ff); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tokyo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">geo</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #ccf2ff, #b3e6ff); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    New York\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">geo</span>\n",
       "</mark>\n",
       ".<br>    </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span\n",
    "from spacy import displacy\n",
    "\n",
    "# Load your BERT model and tokenizer\n",
    "model_path = \"/Users/shakibibnashameem/Documents/Practice/bert/bert-ner/artifacts/v_3/bert_trained/\"\n",
    "tokenizer_path = \"/Users/shakibibnashameem/Documents/Practice/bert/bert-ner/artifacts/v_3/tokenizer/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "# Create a Hugging Face pipeline for NER\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Input text\n",
    "#text = \"John Doe works at Google in Washington. Do you want to go to London? How every today is 1st of December 2024\"\n",
    "#text = \"On January 8, 2025, Alice Johnson from Microsoft traveled to Berlin, Germany, for a conference.\"\n",
    "#text = \"Mark Smith attended the IBM event in Tokyo on July 22, 2023, discussing the latest AI advancements.\"\n",
    "#text = \"The meeting with Sarah Lee at Apple in London will take place on February 5, 2025.\"\n",
    "text = f\"\"\"\n",
    "        The annual summit was held in the breathtaking  Himalayas in Nepal and organized by the United Nations. \n",
    "        Distinguished speakers, including Professor Williams, discussed climate change impacts during the \n",
    "        session on January 8th, with attendees from Tokyo and New York.\n",
    "    \"\"\"\n",
    "# Run the NER pipeline\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "# Convert the output to SpaCy format\n",
    "nlp = spacy.blank(\"en\")  # Create a blank SpaCy model\n",
    "doc = nlp(text)\n",
    "\n",
    "# Create entities\n",
    "ents = []\n",
    "for entity in entities:\n",
    "    start = entity['start']\n",
    "    end = entity['end']\n",
    "    label = entity['entity_group']\n",
    "    span = doc.char_span(start, end, label=label)\n",
    "    if span:\n",
    "        ents.append(span)\n",
    "\n",
    "# Assign entities to the SpaCy Doc\n",
    "doc.ents = ents\n",
    "\n",
    "# Define custom colors for each entity type\n",
    "colors = {\n",
    "    \"PER\": \"linear-gradient(90deg, #e3d8fd, #fbd8f5)\",  \n",
    "    \"ORG\": \"linear-gradient(90deg, #fff4cc, #ffe6b3)\",  \n",
    "    \"GEO\": \"linear-gradient(90deg, #ccf2ff, #b3e6ff)\",  \n",
    "    \"GPE\": \"linear-gradient(90deg, #ccffee, #b3ffdd)\",  \n",
    "    \"TIM\": \"linear-gradient(90deg, #ffcccc, #ffb3b3)\",  \n",
    "    \"O\": \"linear-gradient(90deg, #f2f2f2, #d9d9d9)\"     \n",
    "}\n",
    "\n",
    "\n",
    "# Set the options for visualization\n",
    "options = {\"ents\": list(colors.keys()), \"colors\": colors}\n",
    "\n",
    "# Visualize the entities with SpaCy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True, options=options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Output Image](/Users/shakibibnashameem/Documents/Practice/bert/bert-ner/output.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
