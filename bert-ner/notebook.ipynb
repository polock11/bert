{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import mps\n",
    "from transformers import BertTokenizer, BertForTokenClassification, BertConfig\n",
    "\n",
    "\n",
    "device = 'mps' if mps.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/ner_data.csv'\n",
    "data = pd.read_csv(path, encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2s/t87m92j14g9b1yw35n324r5w0000gn/T/ipykernel_46598/917065080.py:1: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data = data.fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "data = data.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert iob tags to base tag\n",
    "data['base_tag'] = data['Tag'].apply(lambda x: x.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOB tag count\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tag\n",
       "O        887908\n",
       "B-geo     37644\n",
       "B-tim     20333\n",
       "B-org     20143\n",
       "I-per     17251\n",
       "B-per     16990\n",
       "I-org     16784\n",
       "B-gpe     15870\n",
       "I-geo      7414\n",
       "I-tim      6528\n",
       "B-art       402\n",
       "B-eve       308\n",
       "I-art       297\n",
       "I-eve       253\n",
       "B-nat       201\n",
       "I-gpe       198\n",
       "I-nat        51\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = data['Tag'].value_counts()\n",
    "print(\"IOB tag count\")\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique base tag: ['O', 'gpe', 'org', 'tim', 'nat', 'geo', 'per', 'eve', 'art']\n"
     ]
    }
   ],
   "source": [
    "iob_tags = []\n",
    "for t, f in zip(freqs.index, freqs):\n",
    "    iob_tags.append(t)\n",
    "\n",
    "unique_tag =  []\n",
    "for tag in iob_tags:\n",
    "    s = tag.split('-')\n",
    "    unique_tag.append(s[-1])\n",
    "\n",
    "unique_tag = list(set(unique_tag))\n",
    "print(f'Unique base tag: {unique_tag}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "base_tag\n",
       "O      887908\n",
       "geo     45058\n",
       "org     36927\n",
       "per     34241\n",
       "tim     26861\n",
       "gpe     16068\n",
       "art       699\n",
       "eve       561\n",
       "nat       252\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['base_tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# art eve nat is not defined properly, removing them\n",
    "to_remove = ['art','nat','eve']\n",
    "\n",
    "data = data[~data.base_tag.isin(to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>base_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag base_tag\n",
       "0  Sentence: 1      Thousands  NNS   O        O\n",
       "1  Sentence: 1             of   IN   O        O\n",
       "2  Sentence: 1  demonstrators  NNS   O        O\n",
       "3  Sentence: 1           have  VBP   O        O\n",
       "4  Sentence: 1        marched  VBN   O        O"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['Tag'].value_counts().index\n",
    "\n",
    "label2id = {}\n",
    "id2label = {}\n",
    "for idx, label in enumerate(labels):\n",
    "    label2id[label] = idx\n",
    "    id2label[idx] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-geo',\n",
       " 2: 'B-tim',\n",
       " 3: 'B-org',\n",
       " 4: 'I-per',\n",
       " 5: 'B-per',\n",
       " 6: 'I-org',\n",
       " 7: 'B-gpe',\n",
       " 8: 'I-geo',\n",
       " 9: 'I-tim',\n",
       " 10: 'I-gpe'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-geo': 1,\n",
       " 'B-tim': 2,\n",
       " 'B-org': 3,\n",
       " 'I-per': 4,\n",
       " 'B-per': 5,\n",
       " 'I-org': 6,\n",
       " 'B-gpe': 7,\n",
       " 'I-geo': 8,\n",
       " 'I-tim': 9,\n",
       " 'I-gpe': 10}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Sentence'] = data[['Sentence #','Word', \"Tag\"]].groupby(['Sentence #'])[\"Word\"].transform(lambda x: ' '.join(x))\n",
    "data['Word_labels'] = data[['Sentence #','Word', \"Tag\"]].groupby(['Sentence #'])[\"Tag\"].transform(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>base_tag</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Word_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>O O O O O O B-geo O O O O O B-geo O O O O O B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>O O O O O O B-geo O O O O O B-geo O O O O O B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>O O O O O O B-geo O O O O O B-geo O O O O O B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>O O O O O O B-geo O O O O O B-geo O O O O O B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>O O O O O O B-geo O O O O O B-geo O O O O O B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>O O O O O O B-geo O O O O O B-geo O O O O O B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "      <td>geo</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>O O O O O O B-geo O O O O O B-geo O O O O O B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>O O O O O O B-geo O O O O O B-geo O O O O O B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>O O O O O O B-geo O O O O O B-geo O O O O O B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>O O O O O O B-geo O O O O O B-geo O O O O O B-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS    Tag base_tag  \\\n",
       "0  Sentence: 1      Thousands  NNS      O        O   \n",
       "1  Sentence: 1             of   IN      O        O   \n",
       "2  Sentence: 1  demonstrators  NNS      O        O   \n",
       "3  Sentence: 1           have  VBP      O        O   \n",
       "4  Sentence: 1        marched  VBN      O        O   \n",
       "5  Sentence: 1        through   IN      O        O   \n",
       "6  Sentence: 1         London  NNP  B-geo      geo   \n",
       "7  Sentence: 1             to   TO      O        O   \n",
       "8  Sentence: 1        protest   VB      O        O   \n",
       "9  Sentence: 1            the   DT      O        O   \n",
       "\n",
       "                                            Sentence  \\\n",
       "0  Thousands of demonstrators have marched throug...   \n",
       "1  Thousands of demonstrators have marched throug...   \n",
       "2  Thousands of demonstrators have marched throug...   \n",
       "3  Thousands of demonstrators have marched throug...   \n",
       "4  Thousands of demonstrators have marched throug...   \n",
       "5  Thousands of demonstrators have marched throug...   \n",
       "6  Thousands of demonstrators have marched throug...   \n",
       "7  Thousands of demonstrators have marched throug...   \n",
       "8  Thousands of demonstrators have marched throug...   \n",
       "9  Thousands of demonstrators have marched throug...   \n",
       "\n",
       "                                         Word_labels  \n",
       "0  O O O O O O B-geo O O O O O B-geo O O O O O B-...  \n",
       "1  O O O O O O B-geo O O O O O B-geo O O O O O B-...  \n",
       "2  O O O O O O B-geo O O O O O B-geo O O O O O B-...  \n",
       "3  O O O O O O B-geo O O O O O B-geo O O O O O B-...  \n",
       "4  O O O O O O B-geo O O O O O B-geo O O O O O B-...  \n",
       "5  O O O O O O B-geo O O O O O B-geo O O O O O B-...  \n",
       "6  O O O O O O B-geo O O O O O B-geo O O O O O B-...  \n",
       "7  O O O O O O B-geo O O O O O B-geo O O O O O B-...  \n",
       "8  O O O O O O B-geo O O O O O B-geo O O O O O B-...  \n",
       "9  O O O O O O B-geo O O O O O B-geo O O O O O B-...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Word_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>O O O O O O B-geo O O O O O B-geo O O O O O B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Families of soldiers killed in the conflict jo...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O B-per O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They marched from the Houses of Parliament to ...</td>\n",
       "      <td>O O O O O O O O O O O B-geo I-geo O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Police put the number of marchers at 10,000 wh...</td>\n",
       "      <td>O O O O O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The protest comes on the eve of the annual con...</td>\n",
       "      <td>O O O O O O O O O O O B-geo O O B-org I-org O ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  Thousands of demonstrators have marched throug...   \n",
       "1  Families of soldiers killed in the conflict jo...   \n",
       "2  They marched from the Houses of Parliament to ...   \n",
       "3  Police put the number of marchers at 10,000 wh...   \n",
       "4  The protest comes on the eve of the annual con...   \n",
       "\n",
       "                                         Word_labels  \n",
       "0  O O O O O O B-geo O O O O O B-geo O O O O O B-...  \n",
       "1  O O O O O O O O O O O O O O O O O O B-per O O ...  \n",
       "2                O O O O O O O O O O O B-geo I-geo O  \n",
       "3                      O O O O O O O O O O O O O O O  \n",
       "4  O O O O O O O O O O O B-geo O O B-org I-org O ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[[\"Sentence\", \"Word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = {}\n",
    "input_ids = []\n",
    "attention_mask = []\n",
    "labels = []\n",
    "\n",
    "for sent in data[\"Sentence\"]:\n",
    "    encoded_dict =  tokenizer.encode_plus(\n",
    "        sent,\n",
    "        add_special_tokens = True,\n",
    "        max_length = 128,\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True,\n",
    "        return_tensors = 'pt'\n",
    "    )\n",
    "\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_mask.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "for iob_labels in data[\"Word_labels\"]:\n",
    "    l = iob_labels.split(' ')\n",
    "    temp = [label2id[x] for x in l]\n",
    "    temp.insert(0,-100)\n",
    "    temp.insert(len(t)+1, -100)\n",
    "    pad_array = [-100] * (128 - len(temp))\n",
    "    temp.extend(pad_array)\n",
    "    temp = torch.tensor(temp)\n",
    "    labels.append(temp)\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    temp_dict = {}\n",
    "    temp_dict['attention_mask'] = attention_mask[i]\n",
    "    temp_dict['input_ids'] = input_ids[i]\n",
    "    temp_dict['labels'] = labels[i]\n",
    "    item[i] = temp_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47610, 47610, 47610)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids),len(attention_mask), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class MakeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, label2id, tokenizer) -> None:\n",
    "        self.data = data\n",
    "        self.label2id =label2id\n",
    "        self.len = data.shape\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        item = {}\n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "        labels = []\n",
    "\n",
    "        for sent in self.data[\"Sentence\"]:\n",
    "            encoded_dict =  self.tokenizer.encode_plus(\n",
    "                sent,\n",
    "                add_special_tokens = True,\n",
    "                max_length = 128,\n",
    "                pad_to_max_length = True,\n",
    "                return_attention_mask = True,\n",
    "                return_tensors = 'pt'\n",
    "            )\n",
    "\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_mask.append(encoded_dict['attention_mask'])\n",
    "            \n",
    "        for iob_labels in self.data[\"Word_labels\"]:\n",
    "            l = iob_labels.split(' ')\n",
    "            temp = [label2id[x] for x in l]\n",
    "            temp.insert(0,-100)\n",
    "            temp.insert(len(t)+1, -100)\n",
    "            pad_array = [-100] * (128 - len(temp))\n",
    "            temp.extend(pad_array)\n",
    "            temp = torch.tensor(temp)\n",
    "            labels.append(temp)\n",
    "\n",
    "        for i in range(self.len[0]):\n",
    "            temp_dict = {}\n",
    "            temp_dict['attention_mask'] = attention_mask[i]\n",
    "            temp_dict['input_ids'] = input_ids[i]\n",
    "            temp_dict['labels'] = labels[i]\n",
    "            item[i] = temp_dict\n",
    "        \n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class MakeDataset(Dataset):\n",
    "    def __init__(self, data, label2id, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.label2id = label2id\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Pre-tokenize and prepare data\n",
    "        self.tokenized_data = self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        tokenized_data = []\n",
    "\n",
    "        for sentence, word_labels in zip(self.data[\"Sentence\"], self.data[\"Word_labels\"]):\n",
    "            # Tokenize sentence\n",
    "            encoded_dict = self.tokenizer.encode_plus(\n",
    "                sentence,\n",
    "                add_special_tokens = True,\n",
    "                max_length = 128,\n",
    "                pad_to_max_length = True,\n",
    "                return_attention_mask = True,\n",
    "                return_tensors = 'pt'\n",
    "            )\n",
    "\n",
    "            # Prepare labels\n",
    "            labels = word_labels.split(' ')\n",
    "            label_ids = [self.label2id[label] for label in labels]\n",
    "            label_ids = [-100] + label_ids + [-100]  # Add special token labels\n",
    "            label_ids += [-100] * (self.max_length - len(label_ids))  # Pad labels to max_length\n",
    "            label_ids = label_ids[:self.max_length]  # Truncate if necessary\n",
    "\n",
    "            # Add to tokenized data\n",
    "            tokenized_data.append({\n",
    "                \"input_ids\": encoded_dict['input_ids'].squeeze(0),\n",
    "                \"attention_mask\": encoded_dict['attention_mask'].squeeze(0),\n",
    "                \"labels\": torch.tensor(label_ids)\n",
    "            })\n",
    "\n",
    "        return tokenized_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.tokenized_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train_dataset = data.sample(frac=train_size,random_state=200)\n",
    "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shakibibnashameem/Documents/Practice/bert/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MakeDataset(train_dataset, label2id, tokenizer)\n",
    "test_dataset = MakeDataset(test_dataset, label2id, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "MAX_GRAD_NORM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'batch_size' : TRAIN_BATCH_SIZE,\n",
    "    'shuffle' : True,\n",
    "    'num_workers' : 0\n",
    "}\n",
    "\n",
    "test_params = {\n",
    "    'batch_size' : VALID_BATCH_SIZE,\n",
    "    'shuffle' : True,\n",
    "    'num_workers' : 0\n",
    "}\n",
    "\n",
    "training_loader = DataLoader(train_dataset, **train_params)\n",
    "testing_loader = DataLoader(test_dataset, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(label2id), id2label=id2label, label2id=label2id)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "        labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        tr_logits = outputs.logits\n",
    "        \n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            loss_step = tr_loss / nb_tr_steps\n",
    "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
    "           \n",
    "        # compute training accuracy\n",
    "        flattened_targets = labels.view(-1)  # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels)  # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1)  # shape (batch_size * seq_len,)\n",
    "        \n",
    "        # only compute accuracy at active labels\n",
    "        active_accuracy = labels.view(-1) != -100  # shape (batch_size, seq_len)\n",
    "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        tr_labels.extend(labels.cpu().numpy())\n",
    "        tr_preds.extend(predictions.cpu().numpy())\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training loss per 100 training steps: 2.427699089050293\n",
      "Training loss per 100 training steps: 0.9587471392190102\n",
      "Training loss per 100 training steps: 0.8004043713137878\n",
      "Training loss per 100 training steps: 0.7306420120388962\n",
      "Training loss per 100 training steps: 0.6839140895150249\n",
      "Training loss per 100 training steps: 0.6417619716264531\n",
      "Training loss per 100 training steps: 0.6146598805498313\n",
      "Training loss per 100 training steps: 0.5940929532300856\n",
      "Training loss per 100 training steps: 0.5744178736659137\n",
      "Training loss per 100 training steps: 0.55676809682425\n",
      "Training loss per 100 training steps: 0.5429105507591357\n",
      "Training loss per 100 training steps: 0.5293225755527629\n",
      "Training loss per 100 training steps: 0.5159109711343055\n",
      "Training loss per 100 training steps: 0.5068017360120782\n",
      "Training loss per 100 training steps: 0.4964483862145831\n",
      "Training loss per 100 training steps: 0.4842327230006238\n",
      "Training loss per 100 training steps: 0.47692175881284465\n",
      "Training loss per 100 training steps: 0.46791086895944334\n",
      "Training loss per 100 training steps: 0.4613198118231599\n",
      "Training loss per 100 training steps: 0.45460741568468455\n",
      "Training loss per 100 training steps: 0.44839957363806715\n",
      "Training loss per 100 training steps: 0.4405752041770313\n",
      "Training loss per 100 training steps: 0.43373153001444076\n",
      "Training loss per 100 training steps: 0.4293574920379076\n",
      "Training loss per 100 training steps: 0.42444123290223174\n",
      "Training loss per 100 training steps: 0.4187342541773425\n",
      "Training loss per 100 training steps: 0.4134320883418375\n",
      "Training loss per 100 training steps: 0.40901989629061153\n",
      "Training loss per 100 training steps: 0.40384425599924667\n",
      "Training loss per 100 training steps: 0.3988376062967574\n",
      "Training loss per 100 training steps: 0.3948669977190072\n",
      "Training loss per 100 training steps: 0.39140422184753687\n",
      "Training loss per 100 training steps: 0.38702325019255157\n",
      "Training loss per 100 training steps: 0.3834701421996762\n",
      "Training loss per 100 training steps: 0.37922276346773126\n",
      "Training loss per 100 training steps: 0.3761714218351676\n",
      "Training loss per 100 training steps: 0.37232520088467563\n",
      "Training loss per 100 training steps: 0.3688409082106873\n",
      "Training loss per 100 training steps: 0.3653545099597496\n",
      "Training loss per 100 training steps: 0.3623386906131422\n",
      "Training loss per 100 training steps: 0.3591042703978477\n",
      "Training loss per 100 training steps: 0.35655114587107517\n",
      "Training loss per 100 training steps: 0.35384128421214267\n",
      "Training loss per 100 training steps: 0.3515801477973332\n",
      "Training loss per 100 training steps: 0.34896827915798084\n",
      "Training loss per 100 training steps: 0.3461156582794595\n",
      "Training loss per 100 training steps: 0.3439017318284685\n",
      "Training loss per 100 training steps: 0.34141997759448556\n",
      "Training loss per 100 training steps: 0.33924893136003353\n",
      "Training loss per 100 training steps: 0.33684263886252774\n",
      "Training loss per 100 training steps: 0.33432726237284704\n",
      "Training loss per 100 training steps: 0.3317999486352325\n",
      "Training loss per 100 training steps: 0.32935272847762725\n",
      "Training loss per 100 training steps: 0.32734968067303466\n",
      "Training loss per 100 training steps: 0.32532311940971015\n",
      "Training loss per 100 training steps: 0.3236288849599471\n",
      "Training loss per 100 training steps: 0.3211522749972044\n",
      "Training loss per 100 training steps: 0.31939849530447895\n",
      "Training loss per 100 training steps: 0.31773813080556423\n",
      "Training loss per 100 training steps: 0.31570289548996006\n",
      "Training loss per 100 training steps: 0.314091603493766\n",
      "Training loss per 100 training steps: 0.31242919367514205\n",
      "Training loss per 100 training steps: 0.310556631204519\n",
      "Training loss per 100 training steps: 0.3088245723103922\n",
      "Training loss per 100 training steps: 0.30713717339456287\n",
      "Training loss per 100 training steps: 0.30558526605133646\n",
      "Training loss per 100 training steps: 0.3037658773220737\n",
      "Training loss per 100 training steps: 0.3024669117003952\n",
      "Training loss per 100 training steps: 0.30104215603238327\n",
      "Training loss per 100 training steps: 0.29955598573957704\n",
      "Training loss per 100 training steps: 0.29792915947050286\n",
      "Training loss per 100 training steps: 0.29637731108644305\n",
      "Training loss per 100 training steps: 0.2950058431586433\n",
      "Training loss per 100 training steps: 0.29374264082803453\n",
      "Training loss per 100 training steps: 0.2924196994271543\n",
      "Training loss per 100 training steps: 0.2909793627273532\n",
      "Training loss per 100 training steps: 0.2897297744511025\n",
      "Training loss per 100 training steps: 0.2886262640834319\n",
      "Training loss per 100 training steps: 0.28703643763088965\n",
      "Training loss per 100 training steps: 0.28593676407518126\n",
      "Training loss per 100 training steps: 0.28453079193031455\n",
      "Training loss per 100 training steps: 0.28323684863727716\n",
      "Training loss per 100 training steps: 0.2819815884329545\n",
      "Training loss per 100 training steps: 0.2807655160620306\n",
      "Training loss per 100 training steps: 0.27954480399836545\n",
      "Training loss per 100 training steps: 0.2783235807443886\n",
      "Training loss per 100 training steps: 0.2773720693328594\n",
      "Training loss per 100 training steps: 0.2763201366539537\n",
      "Training loss per 100 training steps: 0.275367432065472\n",
      "Training loss per 100 training steps: 0.2741723030119547\n",
      "Training loss per 100 training steps: 0.2733857303767087\n",
      "Training loss per 100 training steps: 0.27220524834195375\n",
      "Training loss per 100 training steps: 0.2712318770211641\n",
      "Training loss per 100 training steps: 0.27045958604131765\n",
      "Training loss per 100 training steps: 0.26960839942767006\n",
      "Training loss per 100 training steps: 0.2687888623902722\n",
      "Training loss epoch: 0.2685721879541966\n",
      "Training accuracy epoch: 0.9166395692775762\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "            labels = batch['labels'].to(device, dtype = torch.long)\n",
    "            \n",
    "            output = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            loss = output.loss\n",
    "            eval_logits = output.logits\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "        \n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "              \n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            \n",
    "            # only compute accuracy at active labels\n",
    "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        \n",
    "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            eval_labels.extend(labels)\n",
    "            eval_preds.extend(predictions)\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    labels = [id2label[id.item()] for id in eval_labels]\n",
    "    predictions = [id2label[id.item()] for id in eval_preds]\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.2171991914510727\n",
      "Validation loss per 100 evaluation steps: 0.16491883393408185\n",
      "Validation loss per 100 evaluation steps: 0.17520182845599727\n",
      "Validation loss per 100 evaluation steps: 0.18514091232899837\n",
      "Validation loss per 100 evaluation steps: 0.18372580567785618\n",
      "Validation loss per 100 evaluation steps: 0.1852459982550492\n",
      "Validation loss per 100 evaluation steps: 0.18562130825999876\n",
      "Validation loss per 100 evaluation steps: 0.18496346324346774\n",
      "Validation loss per 100 evaluation steps: 0.1876130340192929\n",
      "Validation loss per 100 evaluation steps: 0.1878167449637974\n",
      "Validation loss per 100 evaluation steps: 0.18571304383778506\n",
      "Validation loss per 100 evaluation steps: 0.18518013304820455\n",
      "Validation loss per 100 evaluation steps: 0.18390244609012027\n",
      "Validation loss per 100 evaluation steps: 0.18339966008756692\n",
      "Validation loss per 100 evaluation steps: 0.18428015137563838\n",
      "Validation loss per 100 evaluation steps: 0.1846353479253555\n",
      "Validation loss per 100 evaluation steps: 0.18645064472140488\n",
      "Validation loss per 100 evaluation steps: 0.18681307494257296\n",
      "Validation loss per 100 evaluation steps: 0.18807967322740404\n",
      "Validation loss per 100 evaluation steps: 0.1877633966572584\n",
      "Validation loss per 100 evaluation steps: 0.1869439079632414\n",
      "Validation loss per 100 evaluation steps: 0.1870798866101563\n",
      "Validation loss per 100 evaluation steps: 0.18671331190225288\n",
      "Validation loss per 100 evaluation steps: 0.1861005202256277\n",
      "Validation loss per 100 evaluation steps: 0.18566194068421216\n",
      "Validation loss per 100 evaluation steps: 0.18564860690101628\n",
      "Validation loss per 100 evaluation steps: 0.18577059495484308\n",
      "Validation loss per 100 evaluation steps: 0.18595897049394797\n",
      "Validation loss per 100 evaluation steps: 0.18658077213428798\n",
      "Validation loss per 100 evaluation steps: 0.18786445607274369\n",
      "Validation loss per 100 evaluation steps: 0.18785384453985596\n",
      "Validation loss per 100 evaluation steps: 0.18804051978525424\n",
      "Validation loss per 100 evaluation steps: 0.18761665141793674\n",
      "Validation loss per 100 evaluation steps: 0.18840755709134574\n",
      "Validation loss per 100 evaluation steps: 0.18764562254267866\n",
      "Validation loss per 100 evaluation steps: 0.18764148622912707\n",
      "Validation loss per 100 evaluation steps: 0.1872936222240677\n",
      "Validation loss per 100 evaluation steps: 0.18715185445299887\n",
      "Validation loss per 100 evaluation steps: 0.1879763635844947\n",
      "Validation loss per 100 evaluation steps: 0.1879171477089915\n",
      "Validation loss per 100 evaluation steps: 0.18849018568151257\n",
      "Validation loss per 100 evaluation steps: 0.1886280839421662\n",
      "Validation loss per 100 evaluation steps: 0.18823370879946058\n",
      "Validation loss per 100 evaluation steps: 0.18779169587683073\n",
      "Validation loss per 100 evaluation steps: 0.18747529448736266\n",
      "Validation loss per 100 evaluation steps: 0.1874177047883273\n",
      "Validation loss per 100 evaluation steps: 0.187913806000396\n",
      "Validation loss per 100 evaluation steps: 0.1881435155575746\n",
      "Validation Loss: 0.18782913497949597\n",
      "Validation Accuracy: 0.9346579837645678\n"
     ]
    }
   ],
   "source": [
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/shakibibnashameem/Documents/Practice/bert/bert-ner/artifacts/tokenizer/tokenizer_config.json',\n",
       " '/Users/shakibibnashameem/Documents/Practice/bert/bert-ner/artifacts/tokenizer/special_tokens_map.json',\n",
       " '/Users/shakibibnashameem/Documents/Practice/bert/bert-ner/artifacts/tokenizer/vocab.txt',\n",
       " '/Users/shakibibnashameem/Documents/Practice/bert/bert-ner/artifacts/tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model & tokenizer\n",
    "\n",
    "path = '/Users/shakibibnashameem/Documents/Practice/bert/bert-ner/artifacts/'\n",
    "\n",
    "model.save_pretrained(path+'bert_trained')\n",
    "tokenizer.save_pretrained(path+'tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-geo       0.76      0.67      0.71      7492\n",
      "       B-gpe       0.83      0.80      0.82      3189\n",
      "       B-org       0.80      0.46      0.59      4050\n",
      "       B-per       0.77      0.68      0.72      3388\n",
      "       B-tim       0.73      0.69      0.71      4114\n",
      "       I-geo       0.66      0.54      0.60      1479\n",
      "       I-gpe       0.37      0.16      0.22        45\n",
      "       I-org       0.71      0.55      0.62      3379\n",
      "       I-per       0.71      0.83      0.77      3483\n",
      "       I-tim       0.62      0.61      0.61      1337\n",
      "           O       0.96      0.98      0.97    176754\n",
      "\n",
      "    accuracy                           0.93    208710\n",
      "   macro avg       0.72      0.63      0.67    208710\n",
      "weighted avg       0.93      0.93      0.93    208710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(labels, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_label = pd.DataFrame({\n",
    "    'y_true' : labels,\n",
    "    'y_pred' : predictions\n",
    "})\n",
    "\n",
    "base_label['y_true_base'] = base_label[\"y_true\"].apply(lambda x : x.split('-')[-1])\n",
    "base_label['y_pred_base'] = base_label[\"y_pred\"].apply(lambda x : x.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.96      0.98      0.97    176754\n",
      "         geo       0.77      0.67      0.72      8971\n",
      "         gpe       0.83      0.79      0.81      3234\n",
      "         org       0.81      0.54      0.65      7429\n",
      "         per       0.82      0.85      0.83      6871\n",
      "         tim       0.78      0.74      0.76      5451\n",
      "\n",
      "    accuracy                           0.94    208710\n",
      "   macro avg       0.83      0.76      0.79    208710\n",
      "weighted avg       0.94      0.94      0.94    208710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(base_label['y_true_base'], base_label['y_pred_base']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  But Mr. Nikiforov said the file still lacked such important information as the general 's promotion record during the mid-1990s in the midst of the conflict in Bosnia-Herzegovina .\n",
      "Tokenized:  ['but', 'mr', '.', 'nik', '##if', '##oro', '##v', 'said', 'the', 'file', 'still', 'lacked', 'such', 'important', 'information', 'as', 'the', 'general', \"'\", 's', 'promotion', 'record', 'during', 'the', 'mid', '-', '1990s', 'in', 'the', 'midst', 'of', 'the', 'conflict', 'in', 'bosnia', '-', 'herzegovina', '.']\n",
      "Token IDs:  [2021, 2720, 1012, 23205, 10128, 14604, 2615, 2056, 1996, 5371, 2145, 10858, 2107, 2590, 2592, 2004, 1996, 2236, 1005, 1055, 4712, 2501, 2076, 1996, 3054, 1011, 4134, 1999, 1996, 12930, 1997, 1996, 4736, 1999, 9562, 1011, 11453, 1012]\n"
     ]
    }
   ],
   "source": [
    "sent  = data[\"Sentence\"][23421]\n",
    "\n",
    "print(' Original: ', sent)\n",
    "\n",
    "print('Tokenized: ', tokenizer.tokenize(sent))\n",
    "\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "\n",
    "model_path =  \"/Users/shakibibnashameem/Documents/Practice/bert/bert-ner/artifacts/bert_trained/\"\n",
    "tokenizer_path = \"/Users/shakibibnashameem/Documents/Practice/bert/bert-ner/artifacts/tokenizer/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "text = \"This is London, Sam loves to it. But he needs to go to the hospital.\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "predct_token_class = [model.config.id2label[t.item()] for t in predictions[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-org',\n",
       "  'score': np.float32(0.8038998),\n",
       "  'index': 2,\n",
       "  'word': 'golden',\n",
       "  'start': 4,\n",
       "  'end': 10},\n",
       " {'entity': 'I-org',\n",
       "  'score': np.float32(0.5654787),\n",
       "  'index': 3,\n",
       "  'word': 'state',\n",
       "  'start': 11,\n",
       "  'end': 16},\n",
       " {'entity': 'I-org',\n",
       "  'score': np.float32(0.7688067),\n",
       "  'index': 4,\n",
       "  'word': 'warriors',\n",
       "  'start': 17,\n",
       "  'end': 25},\n",
       " {'entity': 'B-gpe',\n",
       "  'score': np.float32(0.9857943),\n",
       "  'index': 7,\n",
       "  'word': 'american',\n",
       "  'start': 33,\n",
       "  'end': 41},\n",
       " {'entity': 'B-geo',\n",
       "  'score': np.float32(0.9810952),\n",
       "  'index': 13,\n",
       "  'word': 'san',\n",
       "  'start': 80,\n",
       "  'end': 83},\n",
       " {'entity': 'I-geo',\n",
       "  'score': np.float32(0.95863223),\n",
       "  'index': 14,\n",
       "  'word': 'francisco',\n",
       "  'start': 84,\n",
       "  'end': 93}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text = \"The Golden State Warriors are an American professional basketball team based in San Francisco.\"\n",
    "\n",
    "classifire = pipeline(\"ner\", model=model_path, tokenizer=tokenizer, device=device)\n",
    "classifire(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer deep dive start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split()):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = data[\"Sentence\"][2242]\n",
    "labels= data[\"Word_labels\"][2242]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Viera took power in a 1980 coup and ruled 19 years until he was ousted during a civil war .\n",
      "B-per I-per O O O O B-tim O O O B-tim O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "print(sentence)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v1_tokenize_and_preserve_labels(sentence, labels, tokenizer):\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    preserved_labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), labels.split()):\n",
    "        \n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "        ext_lbl = [label] * n_subwords\n",
    "\n",
    "        preserved_labels.extend(ext_lbl)\n",
    "\n",
    "    return tokenized_sentence, preserved_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, l = tokenize_and_preserve_labels(sentence, labels, bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mr', '.', 'vie', '##ra', 'took', 'power', 'in', 'a', '1980', 'coup', 'and', 'ruled', '19', 'years', 'until', 'he', 'was', 'ousted', 'during', 'a', 'civil', 'war', '.']\n",
      "['B-per', 'B-per', 'I-per', 'I-per', 'O', 'O', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(s)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = []\n",
    "l = []\n",
    "\n",
    "for word, label in zip(sentence.split(), labels.split()):\n",
    "    \n",
    "    tokenized_word = bt.tokenize(word)\n",
    "    n_subwords = len(tokenized_word)\n",
    "\n",
    "    tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "    ext_lbl = [label] * n_subwords\n",
    "    l.extend(ext_lbl)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mr', '.', 'vie', '##ra', 'took', 'power', 'in', 'a', '1980', 'coup', 'and', 'ruled', '19', 'years', 'until', 'he', 'was', 'ousted', 'during', 'a', 'civil', 'war', '.']\n",
      "['B-per', 'B-per', 'I-per', 'I-per', 'O', 'O', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_sentence)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['british']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_word = bt.tokenize(\"British\")\n",
    "tokenized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Washington\n",
      "['this', 'is', 'washington']\n",
      "['this', 'is', 'washington']\n"
     ]
    }
   ],
   "source": [
    "text  = \"This is Washington\"\n",
    "\n",
    "\n",
    "print(text)\n",
    "bt = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "b_t = bt.tokenize(text)\n",
    "print(b_t)\n",
    "\n",
    "\n",
    "bft = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "b_ft = bft.tokenize(text)\n",
    "print(b_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, label, bt)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['mr', '.'], 'O')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer deep dive end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
