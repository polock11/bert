{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "seed_val = 42\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.mps.manual_seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/shakibibnashameem/Documents/Practice/bert/bert-classification/data/in_domain_train.tsv\"\n",
    "data = pd.read_csv(path, sep=\"\\t\", header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"sentence\", 'label']]\n",
    "df = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "sentence = data.sentence.values\n",
    "label = data.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sentence, label, test_size=.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "\n",
    "for sent in sentence:\n",
    "    s = sent.split(\" \")\n",
    "    l.append(len(s))\n",
    "\n",
    "print(max(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sentences, label, tokenizer, max_lenght):\n",
    "        self.sentences = sentences\n",
    "        self.label = label\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_lenght\n",
    "\n",
    "        self.data = self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        d = []\n",
    "\n",
    "        for sent, label in zip(self.sentences, self.label):\n",
    "             encodings = self.tokenizer.encode_plus(\n",
    "                 sent,\n",
    "                 add_special_tokens = True,\n",
    "                 max_length = self.max_length,\n",
    "                 padding = 'max_length',\n",
    "                 return_attention_mask = True,\n",
    "                 truncation = True,\n",
    "                 return_tensors = 'pt'\n",
    "             )\n",
    "\n",
    "             d.append({\n",
    "                 \"id\" : encodings['input_ids'].squeeze(0),\n",
    "                 \"mask\" : encodings['attention_mask'].squeeze(0),\n",
    "                 \"label\" : torch.tensor(int(label), dtype=torch.long)\n",
    "             })\n",
    "\n",
    "        return d\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MakeDataset(X_train, y_train, tokenizer, 64)\n",
    "test_data = MakeDataset(X_test, y_test, tokenizer, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': tensor([ 101, 3389, 5444, 2005, 3021, 2138, 2010, 2269, 2409, 2032, 2000, 1012,\n",
       "          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]),\n",
       " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'label': tensor(1)}"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.__getitem__(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=32)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels = 2\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 2e-05, eps = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, device, epochs):\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "            \n",
    "            tr_loss = 0\n",
    "            correct_pred = 0\n",
    "            total_pred = 0\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            for _id, batch in enumerate(train_loader):\n",
    "\n",
    "                id = batch['id'].to(device, dtype = torch.long)\n",
    "                mask = batch['mask'].to(device, dtype = torch.long)\n",
    "                label = batch['label'].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(\n",
    "                    input_ids = id,\n",
    "                    attention_mask = mask,\n",
    "                    token_type_ids = None,\n",
    "                    labels = label\n",
    "                )\n",
    "\n",
    "                loss = output.loss\n",
    "                logits = output.logits\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                tr_loss += output.loss.item()\n",
    "\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                correct_pred += (preds == label).sum().item()\n",
    "                total_pred += label.size(0)\n",
    "\n",
    "            \n",
    "            tr_loss /= len(train_loader)\n",
    "\n",
    "            epoch_accuracy = correct_pred / total_pred\n",
    "\n",
    "            print(f\"Epoch: {epoch+1} | Training Loss: {tr_loss:.4f} | Training Accuracy: {epoch_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training Loss: 0.0633 | Training Accuracy: 0.9825\n",
      "Epoch: 2 | Training Loss: 0.0604 | Training Accuracy: 0.9854\n",
      "Epoch: 3 | Training Loss: 0.0611 | Training Accuracy: 0.9836\n",
      "Epoch: 4 | Training Loss: 0.0451 | Training Accuracy: 0.9848\n",
      "Epoch: 5 | Training Loss: 0.0707 | Training Accuracy: 0.9807\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, device,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
